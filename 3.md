[Volver](./index.md)
# Review: Slope One Predictors for Online Rating-Based Collaborative Filtering

El paper plantea 5 objetivos con que el esquema Slope One debe cumplir:

- Fácil de implementar y mantener
- Actualización on the fly
- Queries eficientes
- Lograr predecir con poca información del usuario
- Ser preciso (dentro de lo razonable)

El paper logra transmitir la simplicidad del esquema, y por tanto, la facilidad de implementación y mantención del mismo. Además, explica claramente los 3 esquemas de Slope One planteados (Slope One, Weighted Slope One, Bi-Polar Slope one) y los 4 esquemas con los que se compararán (Pearson [user-user], Per User Average, Adjusted Cosine Item-Based, Bias From Mean).\\

Lamentablemente, la única comparación experimental objetiva que se presenta es el MAE, es decir, la precisión de los esquemas. Dentro de las conclusiones, los autores plantean que su esquema cumple los cinco objetivos planteados, pero sólo se evalúa el último, y sólo se menciona el MAE obtenido sobre usuarios sin información, pero no el método con el que se obtuvo este error.\\

Tampoco se explica cómo observaron que el esquema es fácil de actualizar on the fly, o la comparación de tiempos de generación de predicciones y creación del modelo.

En conclusión, creo que no se detalló lo suficiente sobre el experimento para concluir lo que afirman. Además, me gustaría saber cómo se podría incluir la información que provee el que un usuario haya visto una película, o que haya comenzado a verla sin terminar, y no la haya evaluado. Esto cae fuera del scope de diferencias de evaluación, ya que no hay un número concreto con que comparar. Pero quizás se pueda penalizar la similaridad en estos casos, o incluso, sacar estas películas del set a evaluar.

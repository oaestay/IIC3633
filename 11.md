[Volver](./index.md)
# Review: A survey of active learning in collaborative filtering recommender systems

El artículo trata sobre un sondeo de las técnicas actuales de aprendizaje activo / reforzado que se han aplicado en filtrado colaborativo, es decir, la selección de pocos items dentro de los que el usuario no ha calificado, para mejorar el rendimiento del sistema.

Estas técnicas se evalúan en dos frentes, la personalización de la selección, y el nivel de hibridación (la cantidad de reglas con las que se seleccionan los items).

Los autores comienzan con una introducción a técnicas básicas de recomendación, dan una pincelada al problema de cold start, e introducen el active learning como una solución que ataca la raíz de este problema. También plantean los los aprendizajes pasivos y activos, y destacan que el aprendizaje activo puede ser necesario cuando la información para implementar el sistema es costosa de obtener, y/o es escasa.

A continuación, se enumeran y clasifican múltiples estrategias y métricas, según los criterios antes mencionados, y según sus objetivos. También explican cómo funcionan éstos algoritmos, sus objetivos en detalle, y cuando es necesario, se da un ejemplo para dar mayor claridad, un caso de esto ocurre cuando se explica la diferencia entre los algoritmos que buscan aumentar la variedad de la recomendación, y los que buscan disminuir el error del sistema, mencionando la película Napoleon Dynamite como un ejemplo de ítem controversial en un dataset de películas. Asimismo, cuando se habla de algoritmos que combinan distintas heurísticas de selección, se habla de la correlación de estas, y se muestran ejemplos claros de lo mismo.

La siguiente sección, compara la evaluación de un sistema offline frente a uno online, es decir, utilizando un dataset antes recolectado, o interactuando con usuarios en tiempo real. Esto implica que en un sistema offline, sólo se puede preguntar al usuario por ítems que ya a calificado, lo que tiene consecuencias en las conclusiones de la evaluación. Inmediatamente, se comentan los tipos de evaluación según uno o múltiples usuarios.

A modo de conclusión, se presenta una tabla con el desempeño de las múltiples estrategias anteriores, guidelines a seguir cuando se implementa el aprendizaje activo, y conclusiones fuertes, y fáciles de inferir luego de leer el artículo. A mi parecer, las conclusiones sobre las distintas formas de implementar el aprendizaje activo son las principales.

A mi parecer, es un artículo completo, que parte desde lo fundamental, y entra en detalle en lo que quería explicar, de manera clara. Lamentablemente, creo que dos puntos no fueron explicados como debiese en el trabajo. Primero, no me quedó claro cómo se calificó el desempeño de cada estrategia como 'muy buena', 'buena', etc. Esto no está mencionado dentro del trabajo. Además, se menciona el hecho de que el aprendizaje activo puede resultar un poco invasivo para el usuario, y este puede ignorar, o incluso peor, abandonar la plataforma por la molestia generada de la consulta de ratings de forma reiterada. También se citan un par de trabajos sobre interfaces de usuarios en las conclusiones, pero me hubiese gustado ver guidelines y ejemplos sobre cómo afrontar este problema.

